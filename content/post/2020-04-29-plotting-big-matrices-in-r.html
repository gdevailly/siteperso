---
title: Plotting heatmaps from big matrices in R
author: Guillaume Devailly
date: '2020-04-29'
slug: Plotting-big-matrices-in-r
categories:
    - R
tags:
    - R
    - data visualisaton
header:
    caption: ''
image: ''
math: true
draft: false
---



<p>In genomics, and in many other -omics or big data fields, we often try to
<a href="https://gdevailly.netlify.app/post/how-to-shuffle-ties-after-a-sort/">plot big matrices</a>.
By big matrices, I mean matrices that have more columns and/or rows than pixels in your device.
For example, if we have a 50 column-matrix with one line per gene (~20,000 lines),
then there are likely more lines in
this matrix than pixels in your screen - 1080 vertical pixels in an HD screen
(unless you’re reading this in a fancy future of hyper high definition).</p>
<p>The problem with plotting matrix that have more lines than pixels in the screen is precisely that:
each pixel will have to represent several values from the matrix.
R’s default behaviour in that case might not be optimal for what we intend to show.</p>
<div id="an-example-with-numeric-data" class="section level2">
<h2>An example with numeric data</h2>
<p>Let’s start with the generation of some toy data, to mimic what one can obtain with
<a href="https://gdevailly.netlify.app/post/how-to-shuffle-ties-after-a-sort/">epigenomics data</a>.
I’m trying to generate a column-centered signal, which becomes stronger along the rows, while keeping some randomness. I’ll leave the code for reproducibility reasons, but
you don’t need to understand this part to understand what comes next:</p>
<pre class="r"><code>suppressPackageStartupMessages(library(dplyr))
library(purrr)

Ncol &lt;- 50
genmat &lt;- map(
    1:20000,
    function(i) {
        runif(Ncol) + c(sort(abs(rnorm(50)))[1:25], rev(sort(abs(rnorm(50)))[1:25]))  * i/5000
    }
) %&gt;% do.call(rbind, .)

genmat[1:5, 1:5]
##            [,1]      [,2]       [,3]      [,4]      [,5]
## [1,] 0.24750229 0.8144309 0.31405005 0.2787540 0.8435071
## [2,] 0.44266149 0.1147394 0.28464511 0.6437944 0.7597911
## [3,] 0.11495737 0.6750608 0.04393633 0.5712240 0.2088942
## [4,] 0.16660166 0.5508895 0.75274403 0.7340737 0.9325773
## [5,] 0.07285492 0.4573314 0.09437322 0.1534962 0.4939674

dim(genmat)
## [1] 20000    50</code></pre>
<p>Many R functions can produce a plot from a matrix, such as <code>heatmap()</code>, <code>heatmap.2()</code> or <code>ComplexHeatmap::Heatmap()</code> (or even <code>ggplot2::geom_raster()</code> with some data reshaping). Most of those use the <code>image()</code> function under the hood (it makes a colourful image of the matrix). We will be using mostly this function here.</p>
<pre class="r"><code>oldpar &lt;- par(mar = rep(0.2, 4)) # reducing plot margins
image(
    t(genmat), # image() has some weird opinions about how your matrix will be plotted
    axes = FALSE,
    col = colorRampPalette(c(&quot;white&quot;, &quot;darkorange&quot;, &quot;black&quot;))(30), # our colour palette
    breaks = c(seq(0, 3, length.out = 30), 100) # colour-to-value mapping
)
box() # adding a box around the heatmap</code></pre>
<p><img src="/post/2020-04-29-plotting-big-matrices-in-r_files/figure-html/first_plot-1.png" width="384" /></p>
<p>At first one might think that all is well. We can see some signal, at the centre, stronger on top than on the bottom. However, with 20,000 rows in the matrix, the image should have been much less noisy.
Here we have 20,000 lines in our matrix, and a few hundred pixels in this png image file.
So R must somehow decide how to summarise the values of several cells to draw a single pixel.
It seems that R picks only one cell behind the pixel (randomly? or probably the first or the last one) and use this value to fill the entire pixel, leading to <strong>strong random downsampling</strong>.</p>
<p>A solution could be to make a png file 20,000 pixels high to avoid this downsampling. The resulting file will be a bit too heavy for convenience, and we would need to increase text sizes and margins accordingly.</p>
<p>Some graphical devices (for example <code>pdf()</code> but not <code>png()</code>), will render the <code>image()</code> differently depending on its parameter <code>useRaster</code> (<code>FALSE</code> by default). Putting it to <code>TRUE</code> might help in some situations. According to <a href="https://en.wikipedia.org/wiki/Rasterisation">Wikipedia</a>,
rasterisation <em>is the task of taking an image described in a vector graphics format […] and converting it into a raster image (a series of pixels)</em>. The rasterisation algorithm will try to resume several cells of the matrix in a single pixel. Let’s try it:</p>
<pre class="r"><code>pdf(&quot;../../static/files/big_hm_1.pdf&quot;)
layout(matrix(c(1, 2), nrow = 1)) # side by side plot

# Left plot, no rasterisation
image(
    t(genmat),
    axes = FALSE,
    col = colorRampPalette(c(&quot;white&quot;, &quot;darkorange&quot;, &quot;black&quot;))(30),
    breaks = c(seq(0, 3, length.out = 30), 100),
    main = &quot;Original matrix&quot;
)

# Right plot, with rasterisation
image(
    t(genmat),
    axes = FALSE,
    col = colorRampPalette(c(&quot;white&quot;, &quot;darkorange&quot;, &quot;black&quot;))(30),
    breaks = c(seq(0, 3, length.out = 30), 100),
    useRaster = TRUE,
    main = &quot;With rasterisation&quot;
)

dev.off()
## pdf 
##   2</code></pre>
<p>The resulting pdf file is available <a href="/files/big_hm_1.pdf">here (pdf)</a>. The results vary greatly depending on the <em>software one use to view the pdf file</em>:</p>
<p><img src="/img/posts/hm1_pdf.png" /></p>
<p>Acrobat, Edge and Okular render the file properly: we can see a much finer representation of the data with rasterisation.
Evince and SumatraPDF somehow inverse the rendering, while <em>veiling</em> the non-rastered version.
Firefox’s pdf renderer gives up and refuses to render the rasterised image (at least on Windows 10, it does render it properly on GNU/Linux).</p>
<p>While this pdf file weights 5 Mo, running the same code using the <code>svg()</code> device leads to the creation of a 200 Mo svg file! Nonetheless, it seems that rasterisation works well for svg files too.</p>
<p>The rasterisation tries to summarise all the different values behind each pixel by taking their mean.
We can do the same process ourselves, with two benefits: we get rid of the variability
between the different pdf reader software, and it will work even for non-vector devices such as png,
greatly limiting the final weight of the image.</p>
<p>The idea is to reduce the matrix size <strong>before</strong> making the plot by applying a function (for example, <code>mean()</code>) that will
summarise several cells into one. I’ll suggest this small function (first seen on
<a href="https://cansnippet.bioinfo-fr.net/details.php?id=3">canSnippet</a>):</p>
<pre class="r"><code># reduce matrix size, using a summarising function (default, mean)
redim_matrix &lt;- function(
    mat,
    target_height = 100,
    target_width = 100,
    summary_func = function(x) mean(x, na.rm = TRUE),
    output_type = 0.0, #vapply style
    n_core = 1 # parallel processing
    ) {

    if(target_height &gt; nrow(mat) | target_width &gt; ncol(mat)) {
        stop(&quot;Input matrix must be bigger than target width and height.&quot;)
    }

    seq_height &lt;- round(seq(1, nrow(mat), length.out = target_height + 1))
    seq_width  &lt;- round(seq(1, ncol(mat), length.out = target_width  + 1))

    # complicated way to write a double for loop
    do.call(rbind, parallel::mclapply(seq_len(target_height), function(i) { # i is row
        vapply(seq_len(target_width), function(j) { # j is column
            summary_func(
                mat[
                    seq(seq_height[i], seq_height[i + 1]),
                    seq(seq_width[j] , seq_width[j + 1] )
                    ]
            )
        }, output_type)
    }, mc.cores = n_core))
}

genmatred &lt;- redim_matrix(genmat, target_height = 600, target_width = 50) # 600 is very roughly the pixel height of the image.

genmatred[1:5, 1:5]
##           [,1]      [,2]      [,3]      [,4]      [,5]
## [1,] 0.4530226 0.4911097 0.4927123 0.5302643 0.5561331
## [2,] 0.5263392 0.5138786 0.5324716 0.5354325 0.5050932
## [3,] 0.4196155 0.4887105 0.5238630 0.5183627 0.5296764
## [4,] 0.5024431 0.5015508 0.5155568 0.5537814 0.5318501
## [5,] 0.5121447 0.5533040 0.4882006 0.4877140 0.5222805

dim(genmatred)
## [1] 600  50</code></pre>
<p>Let’s compare plotting the original matrix (left) with the plot of our reduced matrix.</p>
<pre class="r"><code>layout(matrix(c(1, 2), nrow = 1))

# left plot, original matrix
image(
    t(genmat),
    axes = FALSE,
    col = colorRampPalette(c(&quot;white&quot;, &quot;darkorange&quot;, &quot;black&quot;))(30),
    breaks = c(seq(0, 3, length.out = 30), 100),
    main = &quot;Original matrix&quot;
)
box()

# right plot, summarised matrix
image(
    t(genmatred),
    axes = FALSE,
    col = colorRampPalette(c(&quot;white&quot;, &quot;darkorange&quot;, &quot;black&quot;))(30),
    breaks = c(seq(0, 3, length.out = 30), 100),
    main = &quot;Reduced matrix&quot;
)
box()</code></pre>
<p><img src="/post/2020-04-29-plotting-big-matrices-in-r_files/figure-html/test%20redim_matrix-1.png" width="672" /></p>
<pre class="r"><code>
par(oldpar) # restoring margin size to default values</code></pre>
<p>Much better! Paradoxically, we can better grasp the complexity of the data by summarising it before plotting it, instead of letting R handle poorly the overplotting issue!</p>
</div>
<div id="sparse-matrix" class="section level2">
<h2>Sparse matrix</h2>
<p>In some cases, taking the average values might not be the best way to represent our data.
For example, if the matrix is sparse, we might not want to scale down the few isolated values with all those surrounding zeroes.
Instead, taking the maximal value might lead to a more informative representation.</p>
<p>I encountered this case while working on eQTL (a
<a href="https://en.wikipedia.org/wiki/Quantitative_trait_locus">QTL</a> study
were the phenotype is the level of expression of a gene). The aim is to identify
Single Nucleotide Polymorphisms (SNP) that are associated with changes in gene expression level in a tissue.
A statistical test is done for each SNP and each expressed gene, resulting in many p-values.</p>
<p>In this case, we had roughly 20,000 expressed genes and 45,000 SNP, resulting in a 20,000 x 45,000
matrix of p-values. Most of the p-values are not significant, with only a minority being very small
(or big after <code>-log10(p-value)</code> transformation). What we want to visualise is the best p-values
corresponding to the lead SNP. So we are going to reduce the matrix by taking the <code>max()</code> of the
<code>-log10(p-values)</code> instead of their <code>mean()</code>:</p>
<pre class="r"><code># left matrix, we take the mean of the -log10 of the p-values
redim_matrix(
  eqtls,
  target_height = 600, target_width = 600,
  summary_func = function(x) mean(x, na.rm = TRUE),
  n_core = 14
)

# right matrix we take the maximum of the -log10 of the p-values
redim_matrix(
  eqtls,
  target_height = 600, target_width = 600,
  summary_func = function(x) max(x, na.rm = TRUE),
  n_core = 14
)</code></pre>
<p><img src="/img/posts/eqtls.png" /></p>
<p>Lead SNP p-values are better visualised when taking the best p-values instead of averaging them.</p>
<p>I should mention that without reducing the matrix size before the plot, R refuses to render because
of memory saturation.</p>
</div>
<div id="categorical-data" class="section level2">
<h2>Categorical data</h2>
<p>For categorical data, we cannot take the average of values within a pixel, we rather need to average the colours themselves.
I do it bellow on the RGB space, but it might be better to do it on the fancy HCL space ?
Instead of <code>image()</code> we need to use the <code>rasterImage()</code> function to do the plotting.</p>
<pre class="r"><code># some toy data
mycolors &lt;- matrix(c(
  sample(c(&quot;#0000FFFF&quot;, &quot;#FFFFFFFF&quot;, &quot;#FF0000FF&quot;), size = 5000, replace = TRUE, prob = c(2, 1, 1)),
  sample(c(&quot;#0000FFFF&quot;, &quot;#FFFFFFFF&quot;, &quot;#FF0000FF&quot;), size = 5000, replace = TRUE, prob = c(1, 2, 1)),
  sample(c(&quot;#0000FFFF&quot;, &quot;#FFFFFFFF&quot;, &quot;#FF0000FF&quot;), size = 5000, replace = TRUE, prob = c(1, 1, 2))
), ncol = 1)
color_mat &lt;- t(as.matrix(mycolors))

# custom function to average HTML colours
mean_color &lt;- function(mycolors) {
    R     &lt;- strtoi(x = substr(mycolors,2,3), base = 16)
    G     &lt;- strtoi(x = substr(mycolors,4,5), base = 16)
    B     &lt;- strtoi(x = substr(mycolors,6,7), base = 16)
    alpha &lt;- strtoi(x = substr(mycolors,8,9), base = 16)

    return(
        rgb(
            red   = round(mean(R)),
            green = round(mean(G)),
            blue  = round(mean(B)),
            alpha = round(mean(alpha)),
            maxColorValue = 255
        )
    )
}

# Let&#39;s apply the redim_matrix() function using our newly defined mean_color() function:
color_mat_red &lt;- redim_matrix(
  color_mat,
  target_height = 1,
  target_width = 500,
  summary_func = mean_color,
  output_type = &quot;string&quot;
)

# And do the plotting
layout(matrix(c(1, 2), nrow = 2))

# left plot, original matrix
plot(c(0,1), c(0,1), axes = FALSE, type = &quot;n&quot;,  xlab = &quot;&quot;, ylab = &quot;&quot;, xlim = c(0, 1), ylim = c(0,1), xaxs=&quot;i&quot;, yaxs=&quot;i&quot;, main = &quot;Full matrix&quot;)
rasterImage(
    color_mat,
    xleft   = 0,
    xright  = 1,
    ybottom = 0,
    ytop    = 1
)
box()

# right plot, summarised matrix
plot(c(0,1), c(0,1), axes = FALSE, type = &quot;n&quot;,  xlab = &quot;&quot;, ylab = &quot;&quot;, xlim = c(0, 1), ylim = c(0,1), xaxs=&quot;i&quot;, yaxs=&quot;i&quot;, main = &quot;Reduced matrix&quot;)
rasterImage(
    color_mat_red,
    xleft   = 0,
    xright  = 1,
    ybottom = 0,
    ytop    = 1
)
box()</code></pre>
<p><img src="/post/2020-04-29-plotting-big-matrices-in-r_files/figure-html/categorical_data-1.png" width="672" /></p>
<p>Again, the plot looks much better when we solved the overplotting issue ourselves.</p>
</div>
<div id="ggplot2" class="section level1">
<h1>ggplot2</h1>
<p>As far as I can tell, <code>ggplot2</code> is also affected by the overplotting issue, from both <code>geom_tile()</code> and <code>geom_raster</code> (an optimised <code>geom_tile()</code> when all tiles have the same size).</p>
<pre class="r"><code>library(ggplot2)
library(patchwork)

# Wide to long transformation
data_for_ggplot &lt;- as.data.frame(genmat) %&gt;% 
    mutate(row = rownames(.)) %&gt;% 
    tidyr::pivot_longer(-row, names_to = &quot;col&quot;) %&gt;%
    mutate(row = as.numeric(row), col = readr::parse_number(col))
    
# with geom_tile()
p1 &lt;- ggplot(data_for_ggplot, aes(x = col, y = row, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(
      low = &quot;white&quot;, mid = &quot;darkorange&quot;, high = &quot;black&quot;,
      limits = c(0, 3), midpoint = 1.5, oob = scales::squish
  ) +
  labs(title = &quot;geom_tile&quot;) +
  theme_void() +
  theme(legend.position = &quot;none&quot;)

# with geom_raster()
p2 &lt;- ggplot(data_for_ggplot, aes(x = col, y = row, fill = value)) +
  geom_raster() +
  scale_fill_gradient2(
      low = &quot;white&quot;, mid = &quot;darkorange&quot;, high = &quot;black&quot;,
      limits = c(0, 3), midpoint = 1.5, oob = scales::squish
  ) +
  labs(title = &quot;geom_raster&quot;) +
  theme_void() +
  theme(legend.position = &quot;none&quot;)

p1 + p2</code></pre>
<p><img src="/post/2020-04-29-plotting-big-matrices-in-r_files/figure-html/ggplot2-1.png" width="672" /></p>
</div>
<div id="complexheatmap" class="section level1">
<h1>ComplexHeatmap</h1>
<p>ComplexHeatmap is a fantastic <a href="https://www.bioconductor.org/packages/release/bioc/html/ComplexHeatmap.html">Bioconductor package</a> to plot advanced heatmap with annotations everywhere.</p>
<p>Plotting our original matrix suggest that the package suffers from the same overplotting issue, displaying only a sub-sample of the cells, instead of taking the average value on a per-pixel basis.</p>
<pre class="r"><code>suppressPackageStartupMessages(library(ComplexHeatmap))

Heatmap(
  genmat[nrow(genmat):1, ], # putting the top on top
  col = circlize::colorRamp2(c(0, 1.5, 3), c(&quot;white&quot;, &quot;darkorange&quot;, &quot;black&quot;)), # our colour palette
  cluster_rows = FALSE, cluster_columns = FALSE,
  show_heatmap_legend = FALSE,
  column_title = &quot;No rasterisation&quot;
)</code></pre>
<p><img src="/post/2020-04-29-plotting-big-matrices-in-r_files/figure-html/ComplexHeatmap-1.png" width="672" /></p>
<p>The <code>Heatmap()</code> function does have some parameters that suggest using <em>rasterisation</em> in cases of big matrices,
but as far as I can tell, I see no difference concerning the overplotting issue.</p>
<pre class="r"><code>Heatmap(
  genmat[nrow(genmat):1, ],
  col = circlize::colorRamp2(c(0, 1.5, 3), c(&quot;white&quot;, &quot;darkorange&quot;, &quot;black&quot;)),
  cluster_rows = FALSE, cluster_columns = FALSE,
  show_heatmap_legend = FALSE,
  use_raster = TRUE,
  raster_resize = TRUE, raster_device = &quot;png&quot;,
  column_title = &quot;With rasterisation&quot;
)</code></pre>
<p><img src="/post/2020-04-29-plotting-big-matrices-in-r_files/figure-html/ComplexHeatmapRaster-1.png" width="672" /></p>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>In R, reduce your big matrices before plotting them to better see the small details. Else you will have a sub-optimal plots.</p>
<p>The main issue with this process is that you will have to manage manually the decorations (axes, side colour bars, dendrograms, etc.), which is laborious.</p>
<p>Thanks to <a href="https://bioinfo-fr.net/author/mathurin">Mathurin</a>, <a href="https://bioinfo-fr.net/author/kumquat">Kumquat</a> and <a href="https://bioinfo-fr.net/author/_lhtd_">lhtd</a> for proofreading. A French version of this post was published one week early on the blog <a href="https://bioinfo-fr.net/creer-des-heatmaps-a-partir-de-grosses-matrices-en-r">bioinfo-fr.net</a>.</p>
</div>
</div>
